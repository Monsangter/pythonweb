크롤링 기초
크롤링은 무엇이고 기초적 원리는 무엇인가


인터넷과 웹
웹서버와 디비
http와 소켓 
브라우저
웹앱과 api
크롤링 주의사항

-크롤링이란?
웹 크롤러는 조직적, 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램이다.

-크롤링의 역사
검색을 위해 크롤링이 나왔다.
검색엔진에서 검색어에 해당하는 글을 페이지에서 검색에 불러오는 것 처럼.
검색엔진은 크롤링을 통해 자기 서버로 가져온다.

-웹
페이지들이 거미줄같이 각각의 링크로 이루어져 있다.
크롤러는 현페이지에서 갈 수 있는 모든 페이지를 다 순회한다. 그페이지를 수집해 모두 가지고 있다, 분석해서
보여준다.
크롤러는 페이지를 다 본따서 긁어와서 인덱싱한다. 본따게 됨.
파싱은 더 좁은 범위로써 특정페이지에서 특정정보만 가지고 오고 싶을때. ex(네이버 금융페이지의 환율

-크롤링 활용처
사이트에서 원하는 데이터 추출
업무 자동화하기
알림받기
=========
인터넷 웹

-인터넷 하면 떠오르는 것
네이버, 구글과 같은 사이트
그리고 웹 브라우저.

핸드폰에서 와이파이 접속.
와이파이 신호를 통해 공유기. 모뎀(자그마한 라우터)에 연결
이게 지구 반대편에 있는 구글서버에 연결된다.
가는 과정은 바로 연결돼 있는게 아니라 라우터들이 네트워크 식으로
연결되어 있다.

이렇게 서버로부터 데이터를 받아와 보게 될텐데,  http로 보게된다.

크롬의 개발자도구로필요한 웹 지식과플로우들을볼 수 있다.

네트워크에서 녹화를 누르고 새로고침하면 엄청나게 많은 량의 통신을 볼 수있음.

왼쪽하단에 요청량과 용량을 볼 수 있음.
데이터는 많은량의 라우터를 거치기 때문에 작으면 작을 수록 더 빠르고
안전히 갈 수 있다.
데이터는 01 광신호기 때문에 어긋날 수 있고
이 측면에서도 작을 수록 안정성을 위해 좋음.

통신된 데이터를 확인해보면 헤더를 볼 수 있다.
헤더는 메타데이터 의미
request method채ㅐ

요청과 응답헤더도 있다.

accept에는 요청 형식
cookie에는 로그인 정보가 들어간다.

cookie 탈취도 가능.
refere 어떤 주소에서 접근했는가

user-agent 어떤 사용자가 어떤 기기를 통해 접근했는가

doc은 로딩되는 실제 페이지 전체를 의미함.

즉 우리가 리퀘스트라는 헤더를 주면 서버에서 response라는 스트림을 줌.
============
웹서버와 디비

페이지를 새로고침하면 서버와 통신을하게 되고 새로고침을 반복하면 서버에
무리가 간다.

디비는 웹서버의 역할에 핵심이다.

대표적 디비가 엑셀.
컬럼과 로우가 있다.

실제 웹사이트 이용시에는 

db - 서버(미들웨어) - 유저

중간에 서버가 있다.
실제적으론 라우터를 이용해 연결한다.

서버는 미리만들어논 템플릿에 db에서 불러온 정보를 활용해
렌더링한다.

서버는 CRUD를 활용해 데이터 입출력. 유저에게 어떻게 보기 좋게
만들어줄지 생각한다.

예를들어 서버는 유저에게 어떻게 보기 좋은 정보를 갖다줄지, 다양한 로직
을 활용해 정보를 가공하게 된다.

일반적으로 모바일 사이트는 담는 정보가 작기 때문에
로직이나 db가 적을 가능성이 크다. 모바일 크롤링이 좋음.

======

http?

웹서버로 통신할때 어떤 규격을 사용할 것인가?
ftp등의 프로토콜도 있다.

get
user-agent
host
accept-language
accept-encoding
connection

http/1.1 200 OK
user-agent
host
accept-language
accept-encoding
connection
content-type
content-length

http의 구성은
레퀘스트 라인
0개이상의 헤더
빈 라인
바디


request-line = Method sp Request-URI sp http-version crlf

메소드와 요청uri http버전 이렇게 새개로 이루어져 있다.

요청uri는 호스트와 조합됨.

메소드는 요청방법.

헤더는 앞부분
바디는 콜론뒤의 뒷부분

accept랭기지는 꼭 그언어로 받는건 아니지만 서버처리시 도움줌
accept encoding은 암호화. gzip, deflate
커넥션. 요청 유지. 속도처리 자원황룡 측면 중요

콘텐츠 타입. html요청을 할때 post는 보내는건데, 보내는 콘텐츠타입
데이터 바디가 무슨 타입이냐?컨텐츠타입에 값이 많이 필요시 콜론사용.

content-lenth  바디가 이길이 이상 넘어오면 서버는 응답을 안해도 됨. 칼같이 지키지는 않음.

어쨋든 헤더는 이렇게 메타데이터라고 할 수 있다. 로그인 정보가 들어있을 수 잇음.

바디는 비트맵등의 바이너리 명령어로 이루어져있기도 함.

빈라인

post 메소드는 요청을 담아서 보내는 역할 restful 에서는 생성하는 쪽.

get
데이터를 보내지 않고 받기만 하겠다.
head
헤드만 가져올때.
post
데이터를 처리하고 그에 맞는 응답을 받겠다.
put
썻던 글을 수정하는 등
delete
options
쓰일 수 있는지 검사할때


============
http example login
크롤링이란 데이터를 가져오고 조작하는 행위.

로그인으로 권한 검사를 거쳐 정보 접근 제한.

get /money http/1.1

로그인이 돼 있지 않으면 다음과 같은 response
http/1.1 400 unauthorized

http, 상태코드, 미허가
100 -199 정보전달용
200 성공
300 리디렉트
로그인 성공시 다른 페이지로 바로가기 되는 경우
400 클라이언트에러
500 서버에러

post /login http/1.1
user-agent: mozilla/4.0 (compatible; msie5.01; windows nt)

id=apple&password=banana
form-data 형식 html 바디를 보낼 수도 있고 제이슨을 보낼 수도 있음. 컨텐츠 형식과 비슷.

성공시 다음과 같은 반환을 준다.
http/1.1 200 ok
set-cookie: jsessionId=abcd1234
쿠키값 세팅

get /money http/1.1
cookie:jsessionid_abcd1234 쿠키헤더에 방금 받은 값을 그대로 넣어 겟 요청을 보냄.
이제 로그인 정보가 담겨져 요청할 수 있는 것.

하지만 이상태로 쿠키를 주고 받으면 id, jsession 탈취가능.

이 정보를 https에서는 암호화 해 보내게 된다.
rsa키등등의 암호화.

web socket vs http

여기서 소켓은 단순한 os가 아닌 통신에 있어서의 웹소켓을 말한다.

http는 무조건 요청과 응답이 짝을 이루게 된다.
socket 은 요청과 응답이 알아서 자유롭게 오고 간다.
===========
브라우저

웹서버는 디자인을 모른다. 그냥 흰색바탕에 검은색 글씨의 문자열 그대로를 보내게 된다.

브라우저가 이것을 UI 렌더링하게된다.

굉장히 고도화되고 최적하됐기 때문에 게임기술을 사용하기도 한다.
브라우저가 배터리를 많이 먹는 작업이다. 스크롤할때마다 돔엘리먼트(모든사진)들이 새로 렌더링 된다.

옛날에는 넷스케이프 . 구렸음. 요즘엔 브라우저 지원을 통해 발전됨.

편의성,
북마크등

보안,
내페이지가 아닌 다른 페이지 접근시 차단

페이지 방문시 쿠키 차단등에 대한 응답도 브라우저의 기능임.

스크립트실행
브라우저가 없으면 자바스크립트 실행이 되지 못함.
동적크롤링에서 핵심임.

===========

웹,앱 api

웹하나가 포괄적인 역할을 한다.
그리고 그렇게 작동하기위해 api 가 있다.

항상 로딩되어 있는 부분과, 리퀘스트가 필요한 부분이 있음.

즉 UI와 들어가는 데이터를 분리함.

ui를 크롤링하는 경우는 굉장히 드물다.

웹앱에서 ui는 이미 여러색깔 버튼이 숨어있고
서버에서부터 들어오는 데이터를 통해 맞춰서 
웹이 결정함.

즉 ui적 측면을 서버가 결정하냐 앱이 결정하냐 이부분은 가장 핵심적으로 알고 있어야 하는 부분임.

만약 미세먼지 상태를 파악하기 원하는 상태라면 ui. 등의 html을 파싱하는 것이 아니라 주고받는 데이터를 따라하고 흉내내야함. 이걸 api라고 할 수 있음.

-작동과정

페이지 접속 : https://facebook.com
골격만 로딩, 내용은 채우지 않음.
골격 로딩 후 내용을 요청
아래 주소
facebook.com/api/content로 요청

-
페이스북에 접속을하면 서버가 골격을 줌.
그리고 브라우저는 그 골격에 써져 있는대로 서버에 다시한번 요청을 하게 된다.
이떄 이것이 api라고 할 수 있다.

이런 작동과정엔 이유가 있음.
페이지 작업에 있어, 여러사람이 한페이지를 작업하게되며 충돌하게 된다. 즉 
템플릿으로 나눠 작업하게 됨. 모듈화 가능. api는 재사용성에 있어 매우 뛰어나다.

서버사이드 렌더링
서버가 전체를 담당하기 보다는 부분을 담당하게 되면서 여러가지가 좋아짐.

api는 웹앱과 다른건 ui정보라기 보다는
html은 버튼의 색깔 위치등을 명시하는 반면에
api는 안에 채워넣을 데이터만 줌.

좀더 빠르고, 무한스크롤시 필요한 부분만 주고받게 됨.

크롤링할때는 이런 구조들이 편하다.
==========
크롤링 주의사항(네티켓)
잘못하면 불법이다.

사람인 vs 잡코리아
한쪽에서 정보를 크롤링해 사용했다.

여기어때 vs 야놀자
비슷.



공공재라는 판단이 되기도함
뭐 제한해서 사용한다거나.
어쨌건 오용될 수가 있다.

검색엔진이 내 사이트를 긁어가는데 매우 종요한 규칙.
-robotstxt.org

user-agent: *
disallow:/
어떤 유저든 disallow하겠다.
어떤 유저든 메인페이지를 포함해서 크롤링할 수 없다.

user-agent: *
disallow: /cgi-bin/
disallow: /tmp/
disallow: /-joe/
특정페이지만

user-agent: *
disallow:
제한 없음.

네이버에 대해서 www.naver.com/robots.txt 쳐보면
user-agent: *
disallow: /
allow : /$

대부분의 에이전트에 대해서는 안되는데
끝이/일때만. 즉 랜딩 페이지 메인 페이지 일때만.
정규식에서 달러는 끝 ^ 표시는 시작

사실 robots.txt는 사람이 보라고 만든건 아니고 검색엔진이 보라고 만든 거임.
구글은 쳐보면 페이스북이라든지 트위터에 대해 더 노출 잘되도록 설정해놓음.ㅇ

특정사이트에대해 크롤링하면 서버 트래픽을 늘리고 금전적 피해까지 줄 수 잇음.

============


정적크롤링
빠르고 간단한 크롤링

requests 라이브러리
xml,html,json,jsonp,binary
regex
beautifulsoup
css




동적크롤링
셀레늄을 통해 사용자가 접근할 수 있는 환경
눈에보이지 않는 페이지

broswer
selenium

 
